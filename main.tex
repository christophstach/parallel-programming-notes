\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{minted}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{float}
\usepackage{multicol}



\geometry{
    paper=a4paper,
    top=10mm,
    bottom=10mm,
    left=5mm,
    right=5mm
}

\setminted{
    frame=lines,
    framesep=2mm,
    fontsize=\footnotesize,
    breaklines
}

\newcommand{\eqname}[1]{\tag*{#1}}% Tag equation with name


\title{Parallel Programming Overview}
\author{Christoph Stach}
\date{January 2020}

\begin{document}
\begin{multicols}{2}
    


\section{OpenMP}

\begin{listing}[H]
\begin{minted}{cpp}
#include <omp.h>

omp_get_num_procs(); // Total number of threads available
omp_get_thread_num(); // Current Thread
omp_get_num_threads(); // Number of open threads

#pragma omp parallel
#pragma omp parallel num_threads(threadCount)

#pragma omp parallel
{
  #pragma omp sections
  {
    #pragma omp section
       structured_block_1
           ...
    #pragma omp section
       structured_block_2
           ...
  }
}

#pragma omp prallel for
for(...) { ... }

#shared // shared between all threads
#private // copy of the variable without initialization
#firstprivate // gets copy of data before parallel region
#lastprivate // transfers back to non parallel region

#pragma omp critical // für kritische regionen mit shared variablen

#pragma omp parallel for private(val) reduction(+:sum)
for (row = 0; row < Rows; row++) {
  sum += val;
}
\end{minted}
\end{listing}

\noindent Variablen aus anderen Threads dürfen ohne shared nicht verändert werden.

\section{MPI: Message Passing Interface}


\begin{itemize}
    \item MPI ist ein Kommunikationsprotokoll für die Programmierung von Parallelrechnern.
    \item Sowohl Punkt-zu-Punkt- als auch Kollektivkommunikation werden unterstützt.
    \item vor allem für distributed-memory system
    \item ist auch für shared-memory system geeignet
\end{itemize}

\begin{listing}[H]
\begin{minted}[breaklines]{cpp}
#include <mpi.h>

int main(int argc, char **argv) {
    int commSize; // number of processes
    int myRank; // current process rank (ID)

    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &commSize);
    MPI_Comm_rank(MPI_COMM_WORLD, &myRank)
    
    if(myRank != 0) {
        // child processes send data
        // MPI_Probe auf status (tag) testen
        // tag: MPI_FLOAT, MPI_INT, MPI_CHAR
        
        MPI_Send(buffer, count, type, dest, tag, comm); // Point-to-Point blocking
        MPI_ISend(buffer, count, type, dest, tag, comm, request); // Point-to-Point none-blocking
    } else {
        // root process receives data
        for(...commSize) {
            MPI_Recv(buffer, count, type, source, tag, comm, status); // Point-to-Point blocking
            MPI_IRecv(buffer, count, type, source, tag, comm, request); // Point-to-Point none-blocking
        }
    }
    
    
    int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm); // Broadcasting to all processes
    int MPI_Barrier(MPI_Comm comm); // Wait for all processes
    
    MPI_Scatter(void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator); // rsendcount sollte reicvcount sein
    MPI_Gather(void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator);
    
    
    int MPI_Reduce(const void *sendbuf, void *recvbuf, int count, MPI_Datatype datatype,  MPI_Op op, int root, MPI_Comm comm);
    
    MPI_Finalize();
    
    return 0;
}

Achtung mit MPI_Send und MPI_Recv wegen Deadlocks.


\end{minted}
\end{listing}

\section{Cuda}

Grid - Block - Thread. Global memory. Shared shared memory.


\begin{minted}{cpp}
__global__ : executed on the device. callble from host and device
__device__ : executed on the device. callable from host only
__host__ :   executed on the host. callable from host only
\end{minted}


\begin{minted}{cpp}
void myKernelFn(void) {
    uint i = (blockIdx.x * blockDim.x) + threadIdx.x;
    uint j = (blockIdx.y * blockDim.y) + threadIdx.y;
}

__global__
void add_grpu(int n, float* x, flaot* y) {
    uint index = (blockIdx.x * blockDim.x) + threadIdx.x;
    uint stride = blockDim.x * gridDim.x;
    
    for(int i = index; i < n; i += stride {
        y[i] = x[i] + y[i];
    }
}

int main(int argc, char **argv) {    

    myKernelFn <<<numBlocks,threadsPerBlock>>>( /* params for the kernel function */ );       //<<grid,block>>//


    malloc();
    memcpy();
    memset();
    free();

    __host__  __device__ cudaError_t cudaMalloc ( void** devPtr, size_t size ); //     Allocate memory on the device. 
    __host__ cudaError_t cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind ); // Copies data between host and device. 
        Copies data to the given symbol on the device. 
    __host__ cudaError_t cudaMemset ( void* devPtr, int  value, size_t count ); // Initializes or sets device memory to a value. 
    __host__  __device__ cudaError_t cudaFree ( void* devPtr ); // Frees memory on the device. 
    
    cudaDeviceSyncronize();
}
\end{minted}

\section{OpenCL}

\begin{itemize}
    \item command-queue
    \item kernel
    \item work-item
    \item work-group (jede gruppe teilt shared memory)
    \item gesamt menge an work items  = global work size
    \item global memory von allen work items und work groups geteilt
    \item host memory
    \item global memory (kernel speicher, jede instanz eines kernels hat wahlfreien zugriff auf gesammten bereich)
    \item constant memory: kann von kerneln nur gelesen werden
    \item local memory: gruppen  teilen sich diesen speicher
    \item private memory: speicher nur für kernel instanz 
    \item unterschied cuda / opencl: cuda: nvidia compiler wandelt in parallelcode um ; anzahl von blöce und größe pro block; openCL: gesamtanzahl threads ( global size ) und anzahl thrads pro gruppe (local size) 
\end{itemize}

\begin{minted}{cpp}
#ifdef __APPLE
    #include <OpenCL/opencl.h>
#else
    #include <CL/cl.h>
#endif

__kernel void VectorAdd(__global const float* a, __global const float * b, __global float* c, int iNumElements) {
    int iGID = get_global_id(0);
    c[iGID] = a[iGID] + b[iGID];
}    

int main(){
    cl_int_ret;
    cl_uint ret_num_platforms;
    cl_platform_id platform_id
    cl_uint ret_num_devices;
    cl_device_id device_id;
    
    // PseudoCode
    
    clGetPlatformIDs(...);
    clGetDeviceIDs(...);
    
    clCreateContext(...);
    clCreateCommandQueue(...);
    
    program = clCreateProgramWithSource(context, 1,  sourceString);
    clBuildProgram(...);
    
    kernel = clCreateKernel(progra, "VectorAdd", %ret);
    
    srcA = (void*) malloc(sizeof(cl_float) * 1024);
    srcB = malloc(...);
    dest = malloc(...);
    
    // Geräte puffer erstellen
    
    cmDevSrcA = clCreateBuffer(...);
    cmDevSrcB = clCreateBuffer(...);
    cmDevDest = clCreateBuffer(...);
    
     clSetKernelArg(kernel, 0, sizeof(cl_mem), (void*) &cmDevSrcA);
     clSetKernelArg(kernel, 1, sizeof(cl_mem), (void*) &cmDevSrcB);
     clSetKernelArg(kernel, 2, sizeof(cl_mem), (void*) &cmDevDst);
     clSetKernelArg(kernel, 3, sizeof(cl_int), (void*) &numElements);
     
     // In speicher schreiben
     clEnqueueWriteBuffer(command_queue, cmDevSrcA, ....);
     clEnqueueWriteBuffer(command_queue, cmDevSrcB, ....);
     
     clEnqueueNDRangeKernel(command_queue, kernel, 1, NULL, &szGlobalWorkSize, & szLocalWorkSize, 0, NULL, NULL);
     
     // Ergebnis von speicher lesen
     clEnqueueReadBuffer(command_queue, cmDevDst, CL_TRUE, 0, sizeof(cl_float) * szGlobalWorkSize, dst, 0, NULL, NULL);
     
     free(srcA);
     free(srcB);
     free(dst);
     
     clRealseMemObject(cmDevSrcA);
     clRealseMemObject(cmDevSrcB);
     clRealseMemObject(cmDevDst);
     
     clReleaseProgram(...);
     clReleaseKernel(...);
     clReleaseCommandQueue(...);
     clReleaseContext(...);
}
\end{minted}

Rest siehe folien!!

\section{Hadoop}

CAP - Theorem: Consitency, Availability, Partition tolerance

Map-Shuffle-Reduce
\begin{itemize}
    \item Map (Item => key: vlaue): Von jeder Map-Instanz fließen Daten in eventuell verschiedene Zwischenergebnisspeicher.
    \item Shuffle (): Die Zwischenergebnisse werden gemäß der Ausgabeschlüssel, die von der Map-Funktion produziert wurden, neu verteilt, sodass alle
Zwischenergebnisse mit demselben Schlüssel im nächsten Schritt auf demselben Computersystem verarbeitet werden. 
    \item Reduce (): Für jeden Satz an Zwischenergebnissen berechnet jeweils genau ein Reduce-Prozess die vom Nutzer bereitgestellte Reduce-Funktion und damit die Ausgabedaten. Die Reduce-Prozesse ausgeführt.
\end{itemize}

\begin{itemize}
    \item Dateisystem - HDFS (Hadoop Distributed File System)
    \item Resource Manager - YARN (Yet Another Resource Negotiator)
    \item Job- und TaskTracker - MapReduce
    \item HBase: Skalierbare Datenbank auf Hadoop-Clsuter
    \item Hive: Date-Warehouse-Funtionalitäten
    \item Spark: Eine Nachteil von Hadoop ist daß die Daten während die
Berechnungen (bzw. Reduce und Shuffle) immer in Datei (Files) auf
Datenspeicher (HDFS) geschrieben und gelesen sind.
Spark ist eine in-memory Batch Processing Engine, welche
vornehmlich für Machine-Learning-Anwendungen entwickelt wurde.
Das heißt, die Meisten von den Berechnungen finden
Arbeitspeicher statt, ohne immer auf Festspeicher zu schreiben.
\end{itemize}

\section{Spark}

Typische Anwendungsszenarien sind interaktive Datenabfragen aus verteilten Datenbeständen und Verarbeitung von fließenden Daten (Streaming) von Sensoren oder aus dem Finanzbereich. Die besondere Stärke von Spark ist jedoch das maschinelle Lernen
(Machine Learning) mit den Zusätzen MLib (Machine Learning Bibliothek) oder SparkR (R-Bibliotheken direkt unter Spark verwenden).

\begin{itemize}
    \item RDD
    \item MLib
\end{itemize}

\section{NP-Probleme}

\subsection{P: in polynomialer Zeit lösbar}

Eine Sprache oder ein Problem L ist in P, wenn ein Algorithmus oder deterministische Turingmaschine A und ein Polynom p existiert, so dass A bei einer Eingabe der Länge n nach p(n) Schritten spätestens anhält und dann korrekt akzeptiert oder ablehnt.

\begin{itemize}
    \item binäre Suche mit $ O(log n) $
    \item Sorieralgorithmen $ O(n log n) $
    \item Verbinde jeden Knoten mit jedem Knoten eines Graphen verbinde jeden Knoten mit jedem Knoten eines Graphen mit $ O(n^2) $
    \item Matrizenmultiplikation zweier n x n Matrizen mit $ O(n^3) $
\end{itemize}

\subsection{NP: nichtdeterministische polynomiale}

Klasse NP ist die Menge alle Probleme, die ein nichtdeterministischer
Algorithmus oder nichtdeterministische Turingmaschine mit
polynomialer Komplexität lösen kann. \\

Ein Problem ist nicht praktikabel lösbar, wenn es nicht mit den
vorhandenen Ressourcen lösbar ist.
Diese Probleme gehören zur Klasse NP (nicht in polynomialer Zeit
lösbar), z.B.:



\begin{itemize}
    \item Erzeuge alle Binärcodes der Länge $ n $ mit $ O(2^n) $
    \item Traveling Salesman mit $ O(n!) $
    \item Rucksackproblem
    \item Vertex Cover
    \item Damenproblem: Es sollen jeweils acht Damen auf einem Schachbrett so aufgestellt
werden, dass keine zwei Damen einander gemäß ihren in den
Schachregeln definierten Zugmöglichkeiten schlagen können.
    \item Mengenüberdeckungsproblem
Es fragt, ob zu einer Menge U und n Teilmengen S j von U und einer
natürlichen Zahl k kleiner als n eine Vereinigung von k oder weniger
Teilmengen S j existiert, die der Menge U entspricht (Überdeckung).
    \item Ein Problem ist NP-Vollständig (NP-Complete), wenn es zu den
schwierigsten Problemen in der Klasse NP gehört, also sowohl in NP
liegt als auch NP-schwer ist.
    \item NP-Vollständige Probleme sind Entscheidungsprobleme (mit der
Lösung “Ja” oder “Nein”).
\end{itemize}


\subsection{Mögliche Lösungen von NP}

\begin{itemize}
    \item Brute-force
    \item backtracking (trail and error)
    \item branch and bound
    \item greedy algorithm
    \item hill-climbinga or gradient descent
    \item genetic algorithms
    \item simulated annealing
    \item swarm intelligenz
\end{itemize}


\section{Beweisverfahren}

Ein Algorithmus ist eine:

\begin{itemize}
    \item vollständige und eindeutige Verfahrensvorschrift zur Lösung einer bestimmten Klasse von gleichartigen Problemen.
    \item wohldefinierte Rechenvorschrift, die eine Menge von Elementen als Eingabe verwendet und eine Menge von Elementen als Ausgabe erzeugt
    \item beschreibt eine Rechenvorschrift zum Erhalt einer durch die Formulierung eines Problems gegebenen Eingabe-Ausgabe-Beziehung
\end{itemize}

Eigenschaften von Algorithmen

\begin{itemize}
    \item Eingangswerte und Ausgangswerte
    \item Determiniertheit
    \item Effektivivtät
    \item Endlichkeit: Statisch, Dynamisch
    \item Korrektheit
\end{itemize}

Kompliziertheit im ungünstigsten, günstigstens und allegemeinen (mittleren Fall)


\begin{itemize}
    \item Indirekter Beweis
    \item Direkter Beweis
    \item Beweis durch Widerspruch
\end{itemize}



\subsection{Vollständige Induktion}


\subsubsection{Aufgabe 1}

\noindent Zuerst die Aufgabenstellung niederschreiben
\begin{align*}
    \sum_{k=1}^{n}k = \frac{(n + 1) * n}{2}
\end{align*}


\noindent \textbf{Induktionsanfang:} Beweisen das Aussage für $ n  = 1 $ wahr 
\begin{align*}
    \sum_{k=1}^{1}k = 1
\end{align*}

\noindent Rechte Seite ausrechnen:
\begin{align*}
    \frac{(1 + 1) * 1}{2} = 1
\end{align*}


\noindent \textbf{Induktionsbehauptung:} Behaupten das Aussage für $ n = n + 1 $
\begin{align*}
    \sum_{k=1}^{n + 1}k = \frac{((n + 1) + 1) * (n + 1)}{2}
\end{align*}

\noindent \textbf{Induktionschritt:}

\begin{align*}
    \sum_{k=1}^{n + 1}k  = \sum_{k=1}^{n}k + (n + 1) = \frac{(n + 1) * n}{2} +  (n + 1) = \\
    \frac{(n + 1) * n + 2(n + 1)}{2} = \\
    \frac{n^2 + n + 2n + 2}{2} = \\
    \frac{n^2 + 3n + 2}{2}
\end{align*}
\hrule
\begin{align*}
    \frac{((n + 1) + 1) * (n + 1)}{2} = \\
    \frac{(n + 2) * (n + 1)}{2} = \\
    \frac{n^2 + n +2n + 2}{2} = \\
    \frac{n^2 + 3n + 2}{2}
\end{align*}


\end{multicols}
\end{document}
